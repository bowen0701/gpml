{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A (Detailed) Peek into PCA and SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2016/10/05 by Bowen Li; updated: 2020/02/08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "- [What is Principal Component Analysis (PCA)?](#What-is-PCA?)\n",
    "  * [Assumptions for PCA](#Assumptions-for-PCA)\n",
    "  * [Eigendecomposition](#Eigendecomposition)\n",
    "  * [Summary of PCA](#Summary-of-PCA)\n",
    "- [What is Singular Value Decomposition (SVD)?](#What-is-SVD?)\n",
    "  * [Matrix Version of SVD](#Matrix-Version-of-SVD)\n",
    "- [PCA and SVD](#PCA-and-SVD)\n",
    "  * [How Many Pricipal Components We Shall Use?](#How-Many-Pricipal-Components-We-Shall-Use?)\n",
    "  * [Reduced Rank Approximation by SVD](#Reduced-Rank-Approximation-by-SVD)\n",
    "- [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is PCA?\n",
    "\n",
    "The motivation of PCA is to identify the most meaningful basis to re-express data which can maximize the [signal-to-noise ratio (SNR):](https://en.wikipedia.org/wiki/Signal-to-noise_ratio) $\\sigma^2_{signal} / \\sigma^2_{noise}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/pca_snr.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Maximize the signal, measured by the magnitude of variance; large variance corresponds to interesting structure.\n",
    "- Minimize redundancy (i.e. noise), measured by the magnitude of covariance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the observed data be\n",
    "\n",
    "$$\n",
    "\\underset{(m \\times n)}X = \\{x_{ij}\\} =\n",
    "\\begin{bmatrix}\n",
    "x_{11} & \\cdots & x_{1n} \\\\\n",
    "\\vdots &        & \\vdots \\\\ \n",
    "x_{m1} & \\cdots & x_{mn}\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "  x^T_{1.} \\\\\n",
    "  \\vdots \\\\ \n",
    "  x^T_{m.}\n",
    "  \\end{bmatrix}\n",
    "= [x_{.1}, \\cdots, x_{n}]\n",
    "$$\n",
    "\n",
    "where each row, $x^T_{i.}$, represents an example measured on $n$-dimensional features,\n",
    "\n",
    "$$\n",
    "x^T_{i.} = [x_{i1} \\dots x_{in}]\n",
    "$$\n",
    "\n",
    "and each column, $x_{.j}$, represents normalized feature with zero means, $E(x_{.j}) = 0$, and variance one, $Var(x_{.j}) = 1$.\n",
    "\n",
    "$$\n",
    "x_{.j}= \n",
    "\\begin{bmatrix}\n",
    "x_{1j} \\\\\n",
    "\\vdots \\\\ \n",
    "x_{mj}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For notation simplicity, we also denote each row $x^T_{i.}$ by $x^T_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The covariance matrix of $X$ is\n",
    "\n",
    "$$\n",
    "\\underset{(n \\times n)}{\\Sigma_X} = \\frac{1}{m}X^{T}X \n",
    "= \\frac{1}{m}\n",
    "\\begin{bmatrix}\n",
    "x^T_{.1} \\\\\n",
    "\\vdots \\\\ \n",
    "x^T_{.n}\n",
    "\\end{bmatrix}\n",
    "[x_{.1},...,x_{.n}]\n",
    "= \\frac{1}{m}\n",
    "\\begin{bmatrix}\n",
    "x^T_{.1}x_{.1} & \\cdots & x^T_{.1}x_{.n} \\\\\n",
    "\\vdots     &        & \\vdots \\\\ \n",
    "x^T_{.n}x_{.1} & \\cdots & x^T_{.n}x_{.n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Our goal is to find some transformation, $Y = f(X)$, of X such that covariance of $Y$ is a *diagonal* matrix,\n",
    "\n",
    "$$\n",
    "\\underset{(n \\times n)}{\\Sigma_Y} = \\frac{1}{m}Y^{T}Y\n",
    "$$\n",
    "\n",
    "There are many transformations for diagonalizing the convariance matrix $\\Sigma_X$, PCA arguably selects the easiest one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions for PCA\n",
    "- Linear transformation: Let $\\underset{n \\times n}E = [e_1,...,e_n]$,\n",
    "\n",
    "$$\n",
    "Y = XE\n",
    "$$\n",
    "\n",
    "- Large variance have important structure.\n",
    "- The principal components are orthogonal. \n",
    "\n",
    "The last assumption provides an intuitive simplification that makes PCA soluble with linear algebra decomposition techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigendecomposition\n",
    "\n",
    "The [eigendecomposition](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix), or eigenvector decomposition, is also called the [spectral secomposition](https://en.wikipedia.org/wiki/Spectral_theorem). Suppose $Y = XE$, its covariance matrix\n",
    "\n",
    "$$\n",
    "\\Sigma_Y = \\frac{1}{m}Y^TY = \\frac{1}{m}(XE)^T(XE) = E^T \\left(\\frac{1}{m}X^TX \\right)E\n",
    "$$\n",
    "\n",
    "Since $\\frac{1}{m}X^TX$ is a *symmetric* matrix, it can be diagonalized by a matrix of orthonormal eigenvectors using the spectral decomposistion,\n",
    "\n",
    "$$\n",
    "\\frac{1}{m}X^TX = VDV^T\n",
    "$$\n",
    "\n",
    "where $D = diag(d_1,...,d_n)$ is a diagonal matrix with rank-ordered set of eigenvalues, $d_1 \\ge d_2 \\ge ... \\ge d_n$, and $V = [v_1,...,v_n]$ is the matrix of the corresponding eigenvectors with $V^TV=VV^T = I$.\n",
    "\n",
    "*Sketch of proof:* From the spectral decomposition, for all $j$, we have \n",
    "\n",
    "$$\n",
    "\\frac{1}{m}X^T X v_j = d_j v_j\n",
    "$$\n",
    "\n",
    "Then we have $\\frac{1}{m}X^T X V = V D$ which implies $\\frac{1}{m}X^T X = V D V^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Alternative proof:* We can interpret PCA as finding a unit vector $u$, with $\\|u\\| = 1$, which maximizes variance of the projection of all example $x_i$'s onto $u$,\n",
    "\n",
    "$$\n",
    "\\frac{1}{m} \\sum_{i=1}^m (x_i^T u)^2 \n",
    "= \\frac{1}{m} \\sum_{i=1}^m (x_i^T u)^T (x_i^T u)\n",
    "= u^T \\big(\\frac{1}{m} \\sum_{i=1}^m x_i x_i^T \\big) u\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let's digress a little to ask why the variance of the projection of all examples $x_i$'s onto $u$ has form like the above? First, suppose the angle between $x_i$ and $u$ is $\\theta$, then the projection of each example $x_i$ onto $u$ is\n",
    "\n",
    "$$\n",
    "x_i' = x_i cos\\theta\n",
    "= x_i \\frac{x_i^T u}{|x_i||u|}\n",
    "= x_i \\frac{x_i^T u}{|x_i|}\n",
    "$$\n",
    "\n",
    "since $u$ is a unit vector. Then the length of the projection of one example $x_i$'s onto $u$ is\n",
    "\n",
    "$$\n",
    "\\left[ x_i'^T x_i' \\right]^{1/2}\n",
    "= \\left[ \\left( x_i \\frac{x_i^T u}{|x_i|} \\right)^T \\left( x_i \\frac{x_i^T u}{|x_i|} \\right) \\right]^{1/2}\n",
    "= \\left[ \\left( \\frac{x_i^T u}{|x_i|} \\right) x_i^T x_i \\left( \\frac{x_i^T u}{|x_i|} \\right) \\right]^{1/2}\n",
    "= \\left[ \\left(x_i^T u \\right)^2 \\right]^{1/2}\n",
    "= x_i^T u\n",
    "$$\n",
    "\n",
    "since $x_i^T x_i = \\|x_i\\|^2$. Finally, the projection of one example $x_i$'s onto $u$ is the distance $x_i^T u$ from the origin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go back to our optimization problem, we would like to solve\n",
    "\n",
    "$$\n",
    "max_u u^T \\big(\\frac{1}{m} \\sum_{i=1}^m x_i x_i^T \\big) u\n",
    "$$\n",
    "\n",
    "such that $u^T u = 1$. Using the [lagrange multiplier method](https://en.wikipedia.org/wiki/Lagrange_multiplier), the objective becomes\n",
    "\n",
    "$$\n",
    "max_u L(u, \\lambda) = u^T \\big(\\frac{1}{m} \\sum_{i=1}^m x_i x_i^T \\big) u - d (u^T u - 1)\n",
    "$$\n",
    "\n",
    "Take partial derivative with respect to $u$ we have\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial u} L(u, \\lambda) = \\frac{1}{m} X^T X u - d u := 0\n",
    "$$\n",
    "\n",
    "Hence, we are solving $\\frac{1}{m} X^T X u = d u$, and the eigenvector decomposition result follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results we can select $E = V$. Then since $V^TV=VV^T = I$, $\\Sigma_Y$ now is a diagonal matrix\n",
    "\n",
    "$$\n",
    "\\Sigma_Y = E^T \\left( \\frac{1}{m}X^T X \\right) E\n",
    "    = V^T \\left( V D V^T \\right) V = D\n",
    "$$\n",
    "\n",
    "Note that the eigenvalue $d_j$ is the variance of transformed feature $y_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of PCA\n",
    "- Substract off the mean of each feature: $x_j \\leftarrow x_j - \\overline x_j$, and divide the standard deviation: $x_j \\leftarrow x_j/\\sigma_j$\n",
    "- Compute the eigenvectors of covariance matrix, $\\Sigma_X = \\frac{1}{m}X^TX$\n",
    "- The ith principal components of $X$ is the covariance matrix $\\Sigma_X$'s eigenvector $v_i$\n",
    "- The ith diagonal value of the transformed covariance matrix $\\Sigma_Y$ is the variance of $X$ along $v_i$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is SVD?\n",
    "First recall the eigenvector decomposition: $\\frac{1}{m}X^T X v_j = d_j v_j$. Define\n",
    "- Singular values: square root of eigenvalue (variance) $\\lambda_j = \\sqrt{d_j}$, for $j = 1,...,n$.\n",
    "- Matrix $U = [ u_1,...,u_n ]$, with $u_j$ defined by\n",
    "\n",
    "$$\n",
    "u_j = \\frac{1}{\\lambda_j}\\left( \\frac{1}{\\sqrt{m}}X \\right) v_j\n",
    "$$\n",
    "\n",
    "where $v_j$ is the eigenvector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have\n",
    "- (1) $U$ is *orthonormal* matrix:\n",
    "$u_i^T u_j = \n",
    "\\begin{cases} \n",
    "1, \\text{if } i = j \\\\\n",
    "0, \\text{otherwise}\n",
    "\\end{cases}\n",
    "$\n",
    "- (2) $\\lVert X v_i\\rVert = \\lambda_i$\n",
    "\n",
    "*Sketch of proof:* $u_i^T u_j = \\frac{1}{m} \\left( \\frac{1}{\\lambda_i} X v_i \\right)^T \\left( \\frac{1}{\\lambda_j} X v_j \\right) = \\frac{1}{\\lambda_i \\lambda_j} v_i^T \\frac{1}{m} X^T X v_j = \\frac{1}{\\lambda_i \\lambda_j} v_i^T d_j v_j = \\frac{\\lambda_j}{\\lambda_i} v_i^T v_j$. The first result follows. Further, we can obtain the second result similarly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By rewriting we have\n",
    "\n",
    "$$\n",
    "\\left( \\frac{1}{\\sqrt{m}}X \\right) v_j = \\lambda_j u_j\n",
    "$$\n",
    "\n",
    "That is, normalized $X$ multiplied by an eigenvector $v_j$ of covariance of $X$, $\\frac{1}{m} X^T X$, is equal to a scalar $\\lambda_j$ times another vector, $u_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Version of SVD\n",
    "\n",
    "- By constructing a new *diagonal* matrix of singular values, $\\Sigma = diag(\\lambda_1,...,\\lambda_n)$, where $\\lambda_1 \\ge \\lambda_2 \\ge ... \\ge \\lambda_n$.\n",
    "- Define $V = [v_1,...,v_n]$ and $U = [u_1,...,u_n]$.\n",
    "\n",
    "$$\n",
    "\\left( \\frac{1}{\\sqrt{m}}X \\right) V = U \\Sigma,\n",
    "\\frac{1}{\\sqrt{m}}X = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "\n",
    "Hence, *any* arbitrary matrix $\\frac{1}{\\sqrt{m}}X$ can be transformed to\n",
    "- an orthogonal matrix (*rotation*): $U$\n",
    "- a diagonal matrix (*stretch*): $\\Sigma$\n",
    "- another orthogonal matrix (*second rotation*): $V$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA and SVD\n",
    "From SVD, we have\n",
    "\n",
    "$$\n",
    "\\frac{1}{m} X^T X = \\left( \\frac{1}{\\sqrt{m}}X \\right)^T \\left( \\frac{1}{\\sqrt{m}}X \\right)\n",
    "                  = \\left( U \\Sigma V^T \\right)^T \\left( U \\Sigma V^T \\right) \\\\\n",
    "                  = \\left( V \\Sigma U^T U \\Sigma V^T \\right)\n",
    "                  = V \\Sigma^2 V^T \\equiv V D V^T\n",
    "$$\n",
    "\n",
    "That is, squared singular value is equal to variance of $X$ along eigenvector $v_j$, $\\lambda_j^2 = d_j$.\n",
    "\n",
    "### How Many Pricipal Components We Shall Use?\n",
    "- Total variances: $\\sum_{j=1}^n \\lambda_j^2$.\n",
    "- *Scree plot:* Choose the number of principal component by *elbow method.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/pca_scree_plot.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduced Rank Approximation by SVD\n",
    "Recall from SVD, \n",
    "\n",
    "$$\n",
    "\\frac{1}{\\sqrt{m}}X = U \\Sigma V^T = \\sum_{j=1}^n \\lambda_j u_j v_j^T\n",
    "$$\n",
    "\n",
    "Let $s < n = rank(X)$. Then the reduced rank-$s$ least squares approximation to $\\frac{1}{\\sqrt{m}} X$ is\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\sqrt{m}}\\widehat{X} = \\sum_{j=1}^s \\lambda_j u_j v_j^T\n",
    "$$\n",
    "\n",
    "which minimizes the [Frobenius norm](https://en.wikipedia.org/wiki/Matrix_norm),\n",
    "\n",
    "$$\n",
    "\\frac{1}{m} \\sum_{i=1}^m \\sum_{j=1}^n \\left( x_{ij} - \\widehat{x}_{ij} \\right)^2\n",
    "= tr \\left[ \\left(\\frac{1}{\\sqrt{n}} (X - \\widehat{X}) \\right) \\left(\\frac{1}{\\sqrt{n}} (X - \\widehat{X}) \\right)^T \\right]\n",
    "$$\n",
    "\n",
    "over all matrices $\\widehat{X}$ having rank no greater than $s$. This optimization problem can be solved using eigendecomposition as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Cited as:\n",
    "\n",
    "```\n",
    "@article{li2016pcasvd,\n",
    "  title   = \"A (Detailed) Peek into PCA and SVD\",\n",
    "  author  = \"Li, Bowen\",\n",
    "  journal = \"https://bowen0701.github.io/\",\n",
    "  year    = \"2016\",\n",
    "  url     = \"https://bowen0701.github.io/2016/10/05/pca-svd/\"\n",
    "}\n",
    "```\n",
    "\n",
    "*If you notice mistakes and errors in this post, donâ€™t hesitate to contact me at [bowen0701 at gmail dot com] and I would be grateful to be able to correct them.*\n",
    "\n",
    "See you in the next post. :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Shlens (arXiv, 2014). A Tutorial on Principal Component Analysis.\n",
    "\n",
    "[2] Johnson & Wichern (2002). Applied Multivariate Statistical Analysis.## References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
